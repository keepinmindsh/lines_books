# 정리

- 시퀀스 모델 
  - 순차적인 입력 -> 순차적인 출력 
  - 데이터의 현재 값이 이전의 값에 의존한다. 
  - 시퀀스 데이터는 정해진 길이가 없다. 
  - 입력 데이터의 의미는 그 주변 데이터에 의해 결정 


- 컴퓨터는 단어 이해를 불가 
  - 수치화 
    - 원핫 인코딩 
    - 워드 임베딩 

- 순환신경망 (RNN)
  - 이전 시간 단계의 정보를 저장한다. 이를 기반으로 현재 시간 단계의 출력
  - 시간적인 의존성이 강한 데이터를 처리할 수 있다/ 

- 시간 역전파 (BPTT)
- GRU
- LSTM 
  - Long Short Term Memory 
- 양방향 RNN
- 심층 RNN 
- 개체명 인식 (NER)
- 코사인 유사도 
- 워드 임베딩 
  - 아래의 모델을 활용
    - [GloVe](https://github.com/stanfordnlp/GloVe)
    - [Word2Vec](https://www.tensorflow.org/text/tutorials/word2vec)
    - [FastText:Archived](https://github.com/facebookresearch/fastText) 
    - etc 
- 워드 임베딩의 편향 


# 참고 

> [워드임베딩으로 유사도를 구할 때 유의사항](https://velog.io/@pearl1058/%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9-%EC%8B%9C-%EC%9C%A0%EC%82%AC%EB%8F%84%EB%A5%BC-%EA%B5%AC%ED%95%A0-%EB%95%8C-%EC%9C%A0%EC%9D%98%EC%82%AC%ED%95%AD) 참고  

워드 임베딩을 할 때 의미적인 요소가 들어갈 것이라 생각했지만 좋은 모델을 사용했음에도 의미적인 요소는 크게 반영되지 않음. 문장의 구조가 임베딩 성능에 있어서 상당부분 차지함을 알 수 있다.
이처럼 의미적인 요소를 고려하여 유사도를 구할 때는 sentence bert모델을 사용하는게 훨씬 효과적임을 알 수 있다. 또 코사인 유사도와 같은 유사도 알고리즘이 사람이 생각하는 "유사" 라는 개념과 다르게 작용함을 알 수 있다. 사람이 문장이 유사하다고 할 때는 의미가 먼저 떠오르기 마련인데 유사도 알고리즘은 그렇지 못하다.
이처럼 문장의 의미를 비교하는 task를 수행할 때는sentence transformer 계열의 모델을 Contrastive learning 방식에 해당하는 SimCSE 등을 사용해 파인튜닝하고, 그 모델을 사용하여 임베딩 벡터를 생성해 코사인 유사도를 구하는 것이 효과적이다.